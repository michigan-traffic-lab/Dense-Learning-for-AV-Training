{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae45b7b",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede84562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import functools \n",
    "from functools import reduce\n",
    "import shutil\n",
    "from operator import mul\n",
    "from multiprocessing import Process, Manager, Pool\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "import ujson, json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff97092",
   "metadata": {},
   "source": [
    "## 1. Define data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3591159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please give the path to the data folder\n",
    "root_folder = \"/absolute/path/to/data/folder\"\n",
    "\n",
    "# please give the experiment name of the evaluation\n",
    "exp_name = \"Experiment-testing_DateOfRunning\"\n",
    "\n",
    "processed_data_tmp_folder_path = \"processed_data_tmp\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d40d9b1",
   "metadata": {},
   "source": [
    "## 2. Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func_new(fcd_files, index, keyid):\n",
    "    \"\"\"Process the data for the safety critical events analysis.\n",
    "\n",
    "    Args:\n",
    "        fcd_files (list): List of fcd files.\n",
    "        index (int): Index of the process.\n",
    "        keyid (str): Type of the safety-critical event, should be \"crash\" or \"safe\".\n",
    "    \"\"\"\n",
    "    data_info = {\n",
    "        \"safe_id_list\": [], \"safe_weight\": [],\n",
    "        \"crash_id_list\": [], \"crash_weight\": []\n",
    "    }\n",
    "    for file_fcd in tqdm(fcd_files):\n",
    "        # check json files\n",
    "        fcd_id_list = []\n",
    "        with open(file_fcd) as fcd_obj:\n",
    "            for line in fcd_obj:\n",
    "                try:\n",
    "                    iter_info = json.loads(line)\n",
    "                    fcd_id_list.append(int(iter_info[\"original_name\"]))\n",
    "                except:\n",
    "                    print(\"can't load\",file_fcd)\n",
    "        file_json = file_fcd.replace(\".fcd.json\",\".json\")\n",
    "        json_id_list, json_weight_list, json_lineindex_list = [], [], []\n",
    "        json_weight1_list, json_crit0_list, json_crit001_list, json_crit_list = [], [], [], []\n",
    "        json_nade_info, json_nade_info_new = [], []\n",
    "        with open(file_json) as json_obj:\n",
    "            line_index = 0\n",
    "            for line in json_obj:\n",
    "                try:\n",
    "                    iter_info = json.loads(line)\n",
    "                except:\n",
    "                    print(\"can't load\", file_json, line_index)\n",
    "                    continue\n",
    "                for k in iter_info[\"weight_list_step\"]:\n",
    "                    if len(iter_info[\"weight_list_step\"][k]) > 1:\n",
    "                        iter_info[\"weight_list_step\"][k] = [reduce(mul, iter_info[\"weight_list_step\"][k])]\n",
    "                weight_list = np.array(list(iter_info[\"weight_list_step\"].values()))\n",
    "                cum_weight_list = np.cumprod(weight_list)\n",
    "                find_flag = False\n",
    "                for i in range(len(cum_weight_list)):\n",
    "                    if cum_weight_list[i] < 1:\n",
    "                        find_flag = True\n",
    "                        break\n",
    "                if not find_flag:\n",
    "                    pass\n",
    "                if 1:\n",
    "                    init_time_step = list(iter_info[\"weight_list_step\"].keys())[i]\n",
    "                    if init_time_step in iter_info[\"CAV_info\"]:\n",
    "                        json_id_list.append(int(iter_info[\"episode_info\"][\"id\"]))\n",
    "                        json_weight_list.append(np.clip(iter_info[\"weight_episode\"],0,np.inf))\n",
    "                        json_lineindex_list.append(line_index)\n",
    "                        json_weight1_list.append(len(list(iter_info[\"CAV_info\"].keys())))\n",
    "                        crit_list = []\n",
    "                        crit0_list = []\n",
    "                        crit001_list = []\n",
    "                        nade_info = []\n",
    "                        new_nade_info = []\n",
    "                        for time in iter_info[\"CAV_info\"]:\n",
    "                            if iter_info[\"CAV_info\"][time][\"criticality\"] > 0.01:\n",
    "                                crit001_list.append(time)\n",
    "                            if iter_info[\"CAV_info\"][time][\"criticality\"] > 0.0:\n",
    "                                crit0_list.append(time)\n",
    "                            crit_list.append([time,iter_info[\"CAV_info\"][time][\"criticality\"]])\n",
    "                        json_crit0_list.append(crit0_list)\n",
    "                        json_crit001_list.append(crit001_list)\n",
    "                        json_crit_list.append(crit_list)\n",
    "                        for time in iter_info[\"NADE_info\"]:\n",
    "                            nade_info.append([time]+list(iter_info[\"NADE_info\"][time].values())[0])\n",
    "                        json_nade_info.append(nade_info)\n",
    "                        json_nade_info_new.append(new_nade_info)\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                line_index += 1\n",
    "\n",
    "        if set(fcd_id_list)!=set(json_id_list):\n",
    "            print(file_fcd, len(fcd_id_list), len(json_id_list))\n",
    "        inter_ids = set(fcd_id_list).intersection(set(json_id_list))\n",
    "        inter_ids = sorted(list(inter_ids))\n",
    "        for ep_id in inter_ids:\n",
    "            info = [ep_id,\n",
    "                file_json.split(\"/\")[-1].replace(\".json\",\"\"),\n",
    "                fcd_id_list.index(ep_id), \n",
    "                json_lineindex_list[json_id_list.index(ep_id)],\n",
    "                json_weight1_list[json_id_list.index(ep_id)],\n",
    "                json_crit001_list[json_id_list.index(ep_id)],\n",
    "                json_crit0_list[json_id_list.index(ep_id)],\n",
    "                json_nade_info[json_id_list.index(ep_id)],\n",
    "                json_crit_list[json_id_list.index(ep_id)],                \n",
    "            ]\n",
    "            data_info[keyid+\"_id_list\"].append(info)\n",
    "            data_info[keyid+\"_weight\"].append(json_weight_list[json_id_list.index(ep_id)])\n",
    "    print([len(data_info[k]) for k in data_info])\n",
    "    os.makedirs(processed_data_tmp_folder_path, exist_ok=True)\n",
    "    with open(processed_data_tmp_folder_path+f\"/complete_info_offlinecollect-tmp_{keyid}_{str(index)}.json\",'w') as fp:\n",
    "        print(\"saved\",f\"complete_info_offlinecollect-tmp_{keyid}_{str(index)}.json\")\n",
    "        json.dump(data_info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def densify(fcd_dict, init_time_step):\n",
    "    \"\"\"Remove useless information from trajectory data.\n",
    "\n",
    "    Args:\n",
    "        fcd_dict (dict): Raw trajectory data.\n",
    "        init_time_step (str): Initial time step.\n",
    "\n",
    "    Returns:\n",
    "        dict: Processed trajectory data.\n",
    "    \"\"\"\n",
    "    new_result = {\"fcd-export\":{\"timestep\":[]}, \"original_name\":fcd_dict[\"original_name\"]}\n",
    "    for time_info in fcd_dict[\"fcd-export\"][\"timestep\"]:\n",
    "        if round(float(time_info[\"@time\"]),1) >= round(float(init_time_step),1):\n",
    "            new_time_info = {}\n",
    "            new_time_info[\"@time\"] = time_info[\"@time\"]\n",
    "            new_time_info[\"vehicle\"] = []\n",
    "            for veh in time_info[\"vehicle\"]:\n",
    "                new_veh_info = dict(veh)\n",
    "                for theKey in [\"@pos\",\"@lane\",\"@slope\",\"@accelerationLat\"]:\n",
    "                    new_veh_info.pop(theKey)\n",
    "                new_veh_info[\"@type\"] = new_veh_info[\"@type\"].split(\"@\")[0]\n",
    "                new_time_info[\"vehicle\"].append(new_veh_info)\n",
    "            new_result[\"fcd-export\"][\"timestep\"].append(new_time_info)\n",
    "    return new_result\n",
    "\n",
    "def process_fcd_json(root_folder, input_exp, output_exp, fcd_files, index, keyid, offset=0):\n",
    "    \"\"\"Process the trajectory data and save the processed data.\n",
    "\n",
    "    Args:\n",
    "        root_folder (str): Path to the data folder.\n",
    "        input_exp (str): Experiment name of the evaluation.\n",
    "        output_exp (str): Experiment name of the processed data.\n",
    "        fcd_files (list): List of trajectory data files.\n",
    "        index (int): Index of the parallel process.\n",
    "        keyid (str): The type of the data, should be crash or safe.\n",
    "        offset (int, optional): Offset of file index. Defaults to 0.\n",
    "    \"\"\"\n",
    "    num_lines_section = 50\n",
    "    num = 0\n",
    "    \n",
    "    for file_fcd in tqdm(fcd_files):\n",
    "        file_json = file_fcd.replace(\".fcd.json\",\".json\")\n",
    "        if not os.path.exists(file_fcd) or not os.path.exists(file_json):\n",
    "            continue\n",
    "        # print(file_fcd)\n",
    "        # check json files\n",
    "        \n",
    "        json_id_weight_info = {}\n",
    "        \n",
    "        densified_jsoninfo_list = []\n",
    "        json_id_list = []\n",
    "        with open(file_json) as json_obj:\n",
    "            for line_json in json_obj:\n",
    "                iter_info_json = json.loads(line_json)\n",
    "                if len(list(iter_info_json[\"CAV_info\"].keys())) == 0:\n",
    "                    init_time_step = \"0.0\"\n",
    "                else:\n",
    "                    init_time_step = list(iter_info_json[\"CAV_info\"].keys())[0]\n",
    "                    if iter_info_json[\"CAV_info\"][init_time_step][\"criticality\"] == 0:\n",
    "                        del_time_key_list = []\n",
    "                        for t in iter_info_json[\"CAV_info\"]:\n",
    "                            if iter_info_json[\"CAV_info\"][t][\"criticality\"] == 0:\n",
    "                                del_time_key_list.append(t)\n",
    "                            if iter_info_json[\"CAV_info\"][t][\"criticality\"] > 0 and float(t) > float(init_time_step):\n",
    "                                init_time_step = t\n",
    "                                break\n",
    "                        for t in del_time_key_list:\n",
    "                            iter_info_json[\"CAV_info\"].pop(t)\n",
    "                json_id_weight_info[iter_info_json[\"episode_info\"][\"id\"]] = init_time_step\n",
    "                for theKey in [\"weight_step_info\",\"current_weight\",\"crash_decision_info\",\"decision_time_info\",\"drl_epsilon_step_info\",\n",
    "                               \"real_epsilon_step_info\",\"drl_obs_step_info\",\"ndd_step_info\", \"criticality_step_info\", \"cav_mean_speed\", \n",
    "                               \"RSS_rate\",\n",
    "                              ]:\n",
    "                    iter_info_json.pop(theKey)\n",
    "                \n",
    "                densified_jsoninfo_list.append(iter_info_json)\n",
    "                json_id_list.append(iter_info_json[\"episode_info\"][\"id\"])\n",
    "        \n",
    "        densified_fcdinfo_list = []\n",
    "        fcd_id_list = []\n",
    "        with open(file_fcd) as fcd_obj:\n",
    "            for line_fcd in fcd_obj:\n",
    "                iter_info_fcd = json.loads(line_fcd)\n",
    "                fcd_ep_id = int(iter_info_fcd[\"original_name\"])\n",
    "                if fcd_ep_id in json_id_weight_info:\n",
    "                    new_fcd_info = densify(iter_info_fcd, json_id_weight_info[fcd_ep_id])\n",
    "                    densified_fcdinfo_list.append(new_fcd_info)\n",
    "                else:\n",
    "                    densified_fcdinfo_list.append(iter_info_fcd)\n",
    "                fcd_id_list.append(int(iter_info_fcd[\"original_name\"]))\n",
    "                \n",
    "        inter_ids = set(fcd_id_list).intersection(set(json_id_list))\n",
    "        inter_ids = sorted(list(inter_ids))\n",
    "        \n",
    "        num_lines = len(inter_ids)\n",
    "        num += num_lines\n",
    "        num_sections = int(num_lines/num_lines_section)+1\n",
    "        \n",
    "        for i in range(num_sections):\n",
    "            file_index = int(file_fcd.split(\"/\")[-1].split(\".\")[0])\n",
    "            new_file_index = file_index+offset\n",
    "            file_fcd_output = file_fcd.replace(input_exp,output_exp)\n",
    "            file_fcd_output = file_fcd_output.replace(f\"{file_index}.fcd.json\",f\"{new_file_index}_{i}.fcd.json\")\n",
    "            file_json_output = file_json.replace(input_exp,output_exp)\n",
    "            file_json_output = file_json_output.replace(f\"{file_index}.json\",f\"{new_file_index}_{i}.json\")\n",
    "            if i == num_sections-1:\n",
    "                stored_ids = inter_ids[i*num_lines_section:]\n",
    "            else:\n",
    "                stored_ids = inter_ids[i*num_lines_section:(i+1)*num_lines_section]\n",
    "            \n",
    "            if len(stored_ids) > 0:\n",
    "\n",
    "                # filter problematic trajs (different length)\n",
    "                filtered_stored_ids = []\n",
    "                for ids in stored_ids:\n",
    "                    line1 = densified_fcdinfo_list[fcd_id_list.index(ids)]\n",
    "                    line2 = densified_jsoninfo_list[json_id_list.index(ids)]\n",
    "                    time_step_count1 = len(line1[\"fcd-export\"][\"timestep\"])\n",
    "                    time_step_count2 = len(list(line2[\"CAV_info\"].keys()))\n",
    "                    if time_step_count1 != time_step_count2:\n",
    "                        print(\"problem\", line1[\"original_name\"], file_fcd_output, time_step_count1, time_step_count2)\n",
    "                    else:\n",
    "                        filtered_stored_ids.append(ids)\n",
    "                        \n",
    "                with open(file_fcd_output,'w') as fp1:\n",
    "                    for ids in filtered_stored_ids:\n",
    "                        line1 = densified_fcdinfo_list[fcd_id_list.index(ids)]\n",
    "                        line1[\"original_name\"] = str(int(line1[\"original_name\"])+offset*20000)\n",
    "                        json.dump(line1, fp1)\n",
    "                        fp1.write('\\n')\n",
    "                with open(file_json_output,'w') as fp2:\n",
    "                    for ids in filtered_stored_ids:\n",
    "                        line2 = densified_jsoninfo_list[json_id_list.index(ids)]\n",
    "                        line2[\"episode_info\"][\"id\"] += offset*20000\n",
    "                        json.dump(line2, fp2)\n",
    "                        fp2.write('\\n')\n",
    "\n",
    "def process_fcd_json_mp(fcd_files, offset=0):\n",
    "    \"\"\"Process the trajectory data using parallel processing and save the processed data.\n",
    "\n",
    "    Args:\n",
    "        fcd_files (list): List of trajectory data files.\n",
    "        offset (int, optional): Offset of file index. Defaults to 0.\n",
    "    \"\"\"\n",
    "    num_lines_section = 50\n",
    "    example_fcd_file = fcd_files[0]\n",
    "    input_exp = example_fcd_file.split(\"/\")[-3]\n",
    "    output_exp = input_exp+\"/densified_exps\"        \n",
    "    \n",
    "    for file_fcd in tqdm(fcd_files):\n",
    "        file_json = file_fcd.replace(\".fcd.json\",\".json\")\n",
    "        if not os.path.exists(file_fcd) or not os.path.exists(file_json):\n",
    "            continue\n",
    "        # print(file_fcd)\n",
    "        # check json files\n",
    "        \n",
    "        json_id_weight_info = {}\n",
    "        \n",
    "        densified_jsoninfo_list = []\n",
    "        json_id_list = []\n",
    "        with open(file_json) as json_obj:\n",
    "            for line_json in json_obj:\n",
    "                iter_info_json = json.loads(line_json)\n",
    "                if len(list(iter_info_json[\"CAV_info\"].keys())) == 0:\n",
    "                    init_time_step = \"0.0\"\n",
    "                else:\n",
    "                    init_time_step = list(iter_info_json[\"CAV_info\"].keys())[0]\n",
    "                    if iter_info_json[\"CAV_info\"][init_time_step][\"criticality\"] == 0:\n",
    "                        del_time_key_list = []\n",
    "                        for t in iter_info_json[\"CAV_info\"]:\n",
    "                            if iter_info_json[\"CAV_info\"][t][\"criticality\"] == 0:\n",
    "                                del_time_key_list.append(t)\n",
    "                            if iter_info_json[\"CAV_info\"][t][\"criticality\"] > 0 and float(t) > float(init_time_step):\n",
    "                                init_time_step = t\n",
    "                                break\n",
    "                        for t in del_time_key_list:\n",
    "                            iter_info_json[\"CAV_info\"].pop(t)\n",
    "                json_id_weight_info[iter_info_json[\"episode_info\"][\"id\"]] = init_time_step\n",
    "                for theKey in [\"weight_step_info\",\"current_weight\",\"crash_decision_info\",\"decision_time_info\",\"drl_epsilon_step_info\",\n",
    "                               \"real_epsilon_step_info\",\"drl_obs_step_info\",\"ndd_step_info\", \"criticality_step_info\", \"cav_mean_speed\", \n",
    "                               \"RSS_rate\",\n",
    "                              ]:\n",
    "                    iter_info_json.pop(theKey)\n",
    "                \n",
    "                densified_jsoninfo_list.append(iter_info_json)\n",
    "                json_id_list.append(iter_info_json[\"episode_info\"][\"id\"])\n",
    "        \n",
    "        densified_fcdinfo_list = []\n",
    "        fcd_id_list = []\n",
    "        with open(file_fcd) as fcd_obj:\n",
    "            for line_fcd in fcd_obj:\n",
    "                iter_info_fcd = json.loads(line_fcd)\n",
    "                fcd_ep_id = int(iter_info_fcd[\"original_name\"])\n",
    "                if fcd_ep_id in json_id_weight_info:\n",
    "                    new_fcd_info = densify(iter_info_fcd, json_id_weight_info[fcd_ep_id])\n",
    "                    densified_fcdinfo_list.append(new_fcd_info)\n",
    "                else:\n",
    "                    densified_fcdinfo_list.append(iter_info_fcd)\n",
    "                fcd_id_list.append(int(iter_info_fcd[\"original_name\"]))\n",
    "                \n",
    "        inter_ids = set(fcd_id_list).intersection(set(json_id_list))\n",
    "        inter_ids = sorted(list(inter_ids))\n",
    "        \n",
    "        num_lines = len(inter_ids)\n",
    "        num_sections = int(num_lines/num_lines_section)+1\n",
    "        \n",
    "        for i in range(num_sections):\n",
    "            file_index = int(file_fcd.split(\"/\")[-1].split(\".\")[0])\n",
    "            new_file_index = file_index+offset\n",
    "            file_fcd_output = file_fcd.replace(input_exp,output_exp)\n",
    "            file_fcd_output = file_fcd_output.replace(f\"{file_index}.fcd.json\",f\"{new_file_index}_{i}.fcd.json\")\n",
    "            file_json_output = file_json.replace(input_exp,output_exp)\n",
    "            file_json_output = file_json_output.replace(f\"{file_index}.json\",f\"{new_file_index}_{i}.json\")\n",
    "            if i == num_sections-1:\n",
    "                stored_ids = inter_ids[i*num_lines_section:]\n",
    "            else:\n",
    "                stored_ids = inter_ids[i*num_lines_section:(i+1)*num_lines_section]\n",
    "            \n",
    "            if len(stored_ids) > 0:\n",
    "                # filter problematic trajs (different length)\n",
    "                filtered_stored_ids = []\n",
    "                for ids in stored_ids:\n",
    "                    line1 = densified_fcdinfo_list[fcd_id_list.index(ids)]\n",
    "                    line2 = densified_jsoninfo_list[json_id_list.index(ids)]\n",
    "                    time_step_count1 = len(line1[\"fcd-export\"][\"timestep\"])\n",
    "                    time_step_count2 = len(list(line2[\"CAV_info\"].keys()))\n",
    "                    if time_step_count1 != time_step_count2:\n",
    "                        print(\"problem\", line1[\"original_name\"], file_fcd_output, time_step_count1, time_step_count2)\n",
    "                    else:\n",
    "                        filtered_stored_ids.append(ids)\n",
    "                with open(file_fcd_output,'w') as fp1:\n",
    "                    for ids in filtered_stored_ids:\n",
    "                        line1 = densified_fcdinfo_list[fcd_id_list.index(ids)]\n",
    "                        line1[\"original_name\"] = str(int(line1[\"original_name\"])+offset*20000)\n",
    "                        json.dump(line1, fp1)\n",
    "                        fp1.write('\\n')\n",
    "                with open(file_json_output,'w') as fp2:\n",
    "                    for ids in filtered_stored_ids:\n",
    "                        line2 = densified_jsoninfo_list[json_id_list.index(ids)]\n",
    "                        line2[\"episode_info\"][\"id\"] += offset*20000\n",
    "                        json.dump(line2, fp2)\n",
    "                        fp2.write('\\n')\n",
    "    return f\"finished {fcd_files[0]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c089d1c",
   "metadata": {},
   "source": [
    "### Step 1: Remove unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18370016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Densify fcd files\n",
    "out_exp = exp_name+\"/densified_exps\"\n",
    "output_space = [os.path.join(root_folder,out_exp,\"crash\"),os.path.join(root_folder,out_exp,\"tested_and_safe\")]\n",
    "for fo in output_space:\n",
    "    os.makedirs(fo, exist_ok=True)\n",
    "safe_folder = os.path.join(root_folder, exp_name, \"tested_and_safe\")\n",
    "# safe_fcd_files = sorted(glob.glob(safe_folder+\"/*.fcd.json\"))\n",
    "safe_fcd_files = []\n",
    "for i in range(1000):\n",
    "    safe_fcd_files.append(os.path.join(safe_folder,f\"{i}.fcd.json\"))\n",
    "print(len(safe_fcd_files))\n",
    "# print(safe_fcd_files)\n",
    "crash_folder = os.path.join(root_folder, exp_name, \"crash\")\n",
    "crash_fcd_files = sorted(glob.glob(crash_folder+\"/*.fcd.json\"))\n",
    "print(len(crash_fcd_files))\n",
    "num_each = 100\n",
    "\n",
    "process_fcd_json(root_folder, exp_name, out_exp, crash_fcd_files, 0, \"crash\", offset=0)\n",
    "\n",
    "split_safe_fcd_files = [safe_fcd_files[i*num_each:(i+1)*num_each] for i in range(10)]\n",
    "print(len(split_safe_fcd_files))\n",
    "process_fcd_json_mp(split_safe_fcd_files[0])\n",
    "pool = Pool(5)\n",
    "for result in pool.imap_unordered(process_fcd_json_mp, split_safe_fcd_files):\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3440988",
   "metadata": {},
   "source": [
    "### Step 2: Analyze the safety critical events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543392b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder = os.path.join(root_folder, out_exp)\n",
    "safe_folder = os.path.join(folder, \"tested_and_safe\")\n",
    "safe_fcd_files = sorted(glob.glob(safe_folder+\"/*.fcd.json\"))\n",
    "crash_folder = os.path.join(folder, \"crash\")\n",
    "crash_fcd_files = sorted(glob.glob(crash_folder+\"/*.fcd.json\"))\n",
    "print(len(crash_fcd_files), len(safe_fcd_files))\n",
    "p_list = []\n",
    "num_each = int(len(safe_fcd_files)/10)+1\n",
    "for i in range(10):\n",
    "    if i == 9:\n",
    "        files_list = safe_fcd_files[i*num_each:]\n",
    "    else:\n",
    "        files_list = safe_fcd_files[i*num_each:(i+1)*num_each]\n",
    "    p = Process(target=process_func_new, args=(files_list, i, \"safe\"))\n",
    "    p_list.append(p)\n",
    "p = Process(target=process_func_new, args=(crash_fcd_files, 0, \"crash\"))\n",
    "p_list.append(p)\n",
    "for p_ind in p_list:\n",
    "    p_ind.start()\n",
    "for p_ind in p_list:\n",
    "    p_ind.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8cefc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_training_range_crash_new_nnmetric(crit_info, nade_info):\n",
    "    \"\"\"Find the interesting time step range for the crash event.\n",
    "\n",
    "    Args:\n",
    "        crit_info (list): List of criticality information.\n",
    "        nade_info (list): List of NADE information.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Time step range.\n",
    "    \"\"\"\n",
    "    threshold = 0.9\n",
    "    t_list = []\n",
    "    crit_list = []\n",
    "    for info in crit_info:\n",
    "        t_list.append(info[0])\n",
    "        crit_list.append(info[1])\n",
    "    if crit_list[-1] < 0.001 and crit_list[-2] > 0:\n",
    "        crit_list[-1] = 1\n",
    "    lane_change_info_origin = []\n",
    "    lane_change_info = []\n",
    "    lane_change_crit = []\n",
    "    for i in range(len(t_list)):\n",
    "        if crit_list[i] >= threshold and min(crit_list[i:]) >= 0.001:\n",
    "            return t_list[i], t_list[-1]\n",
    "    return None, None\n",
    "\n",
    "def find_training_range_safe_new_nnmetric(crit_info,nade_info,debug_flag=False):\n",
    "    \"\"\"Find the interesting time step range for the safe event.\n",
    "\n",
    "    Args:\n",
    "        crit_info (list): List of criticality information.\n",
    "        nade_info (list): List of NADE information.\n",
    "        debug_flag (bool, optional): Debug flag. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Time step range.\n",
    "    \"\"\"\n",
    "    threshold = 0.9\n",
    "    t_list = []\n",
    "    crit_list = []\n",
    "    for info in crit_info:\n",
    "        t_list.append(info[0])\n",
    "        crit_list.append(info[1])\n",
    "    if crit_list[-1] == 0 and crit_list[-2] > 0:\n",
    "        crit_list[-1] = 1\n",
    "    lane_change_info_origin = []\n",
    "    lane_change_info = []\n",
    "    lane_change_crit = []\n",
    "    for i in range(len(t_list)):\n",
    "        if crit_list[i] >= threshold:\n",
    "            for j in range(i+1, len(t_list)):\n",
    "                if debug_flag:\n",
    "                    print(i,j,min(crit_list[i:j]),min(crit_list[i:min(j+1,len(t_list))]))\n",
    "                if min(crit_list[i:j]) >= 0.001:\n",
    "                    if min(crit_list[i:j+1]) < 0.001 or j+1 == len(t_list):\n",
    "                        return t_list[i], t_list[j]\n",
    "    return None, None\n",
    "    \n",
    "def debug(info):\n",
    "    \"\"\"Debug the criticality information.\n",
    "\n",
    "    Args:\n",
    "        info (list): List of criticality information.\n",
    "    \"\"\"\n",
    "    print(info[0])\n",
    "    t_list = []\n",
    "    crit_list = []\n",
    "    for ind in info[8]:\n",
    "        t_list.append(float(ind[0]))\n",
    "        crit_list.append(ind[1])\n",
    "    nade_info = info[7]\n",
    "    lane_change_info = []\n",
    "    lane_change_crit = []\n",
    "    for ind in nade_info:\n",
    "        if ind[1] in [0,1]:\n",
    "            if lane_change_info == [] or float(ind[0])-lane_change_info[-1] >= 1:\n",
    "                lane_change_info.append(float(ind[0]))\n",
    "                if ind[2] > 0:\n",
    "                    lane_change_crit.append(\"red\")\n",
    "                else:\n",
    "                    lane_change_crit.append(\"blue\")\n",
    "    plt.figure(dpi=100)\n",
    "    plt.plot(t_list,crit_list)\n",
    "    for t in lane_change_info:\n",
    "        plt.plot([t]*2,[0,1],c=lane_change_crit[lane_change_info.index(t)])\n",
    "    plt.plot(t_list,[0]*len(t_list),\"--\",c=\"k\",alpha=0.5)\n",
    "    plt.plot(t_list,[0.01]*len(t_list),\"--\",c=\"k\",alpha=0.5)\n",
    "    plt.plot(t_list,[0.5]*len(t_list),\"--\",c=\"k\",alpha=0.5)\n",
    "    plt.plot(t_list,[0.9]*len(t_list),\"--\",c=\"k\",alpha=0.5)\n",
    "    plt.xlabel(\"time (s)\")\n",
    "    plt.ylabel(\"criticality\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob(processed_data_tmp_folder_path+\"/complete_info_offlinecollect-tmp_*\"))\n",
    "# print(files)\n",
    "num_crash = 0\n",
    "num_safe = 0\n",
    "\n",
    "data_info = {\n",
    "    \"safe_id_list\": [], \"safe_weight\": [], \"safe_ep_info\": [],\n",
    "    \"crash_id_list\": [], \"crash_weight\": [], \"crash_ep_info\": [],\n",
    "}\n",
    "\n",
    "int_index = 5\n",
    "output_file = \"offline_av_alldata.json\"\n",
    "critic_len_list = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    print(file)\n",
    "    with open(file) as fp:\n",
    "        json_obj = json.load(fp)\n",
    "    for i in range(len(json_obj[\"safe_id_list\"])):\n",
    "        info = json_obj[\"safe_id_list\"][i]\n",
    "        if len(info[int_index]) != 0:\n",
    "            re = find_training_range_safe_new_nnmetric(info[8],info[7])\n",
    "            if re[0] is None:\n",
    "                num_safe+=1\n",
    "            else:\n",
    "                info_out = [\n",
    "                    info[0], info[1], info[2], info[3],\n",
    "                    int(float(re[0])*10), int(float(re[1])*10)\n",
    "                ]\n",
    "                data_info[\"safe_id_list\"].append(info_out)\n",
    "                data_info[\"safe_weight\"].append(json_obj[\"safe_weight\"][i])\n",
    "                data_info[\"safe_ep_info\"].append(tuple(info_out+[json_obj[\"safe_weight\"][i]]))\n",
    "               \n",
    "    for i in range(len(json_obj[\"crash_id_list\"])):\n",
    "        info = json_obj[\"crash_id_list\"][i]\n",
    "        if len(info[int_index]) != 0:\n",
    "            re = find_training_range_crash_new_nnmetric(info[8],info[7])\n",
    "            if re[0] is None:\n",
    "                num_crash+=1\n",
    "                print(\"remove\")\n",
    "                debug(info)\n",
    "            else:\n",
    "                info_out = [\n",
    "                    info[0], info[1], info[2], info[3],\n",
    "                    int(float(re[0])*10), int(float(re[1])*10)\n",
    "                ]\n",
    "                data_info[\"crash_id_list\"].append(info_out)\n",
    "                data_info[\"crash_weight\"].append(json_obj[\"crash_weight\"][i])\n",
    "                data_info[\"crash_ep_info\"].append(tuple(info_out+[json_obj[\"crash_weight\"][i]]))\n",
    "    \n",
    "                \n",
    "print(num_safe, num_crash)\n",
    "print(np.mean(critic_len_list))\n",
    "with open(folder+f\"/{output_file}\", 'w') as fp:\n",
    "    ujson.dump(data_info, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove episode with length smaller than 1\n",
    "json_file_path = folder+\"/offline_av_alldata.json\"\n",
    "with open(json_file_path) as fp:\n",
    "    data_info_origin = ujson.load(fp)\n",
    "    print(len(data_info_origin[\"crash_id_list\"]))\n",
    "    print(data_info_origin.keys())\n",
    "    print(len(data_info_origin[\"safe_id_list\"]))\n",
    "    partial_data_info_origin = {}\n",
    "    partial_data_info_origin[\"crash_id_list\"] = []\n",
    "    partial_data_info_origin[\"crash_weight\"] = []\n",
    "    partial_data_info_origin[\"crash_ep_info\"] = []\n",
    "    partial_data_info_origin[\"safe_id_list\"] = []\n",
    "    partial_data_info_origin[\"safe_weight\"] = []\n",
    "    partial_data_info_origin[\"safe_ep_info\"] = []\n",
    "    for i in range(len(data_info_origin[\"crash_id_list\"])):\n",
    "        if data_info_origin[\"crash_id_list\"][i][5]-data_info_origin[\"crash_id_list\"][i][4]>=2:\n",
    "            partial_data_info_origin[\"crash_id_list\"].append(data_info_origin[\"crash_id_list\"][i])\n",
    "            partial_data_info_origin[\"crash_weight\"].append(data_info_origin[\"crash_weight\"][i])\n",
    "            partial_data_info_origin[\"crash_ep_info\"].append(data_info_origin[\"crash_ep_info\"][i])\n",
    "        else:\n",
    "            print(data_info_origin[\"crash_id_list\"][i])\n",
    "    for i in range(len(data_info_origin[\"safe_id_list\"])):\n",
    "        if data_info_origin[\"safe_id_list\"][i][5]-data_info_origin[\"safe_id_list\"][i][4]>=2:\n",
    "            partial_data_info_origin[\"safe_id_list\"].append(data_info_origin[\"safe_id_list\"][i])\n",
    "            partial_data_info_origin[\"safe_weight\"].append(data_info_origin[\"safe_weight\"][i])\n",
    "            partial_data_info_origin[\"safe_ep_info\"].append(data_info_origin[\"safe_ep_info\"][i])\n",
    "    print(len(partial_data_info_origin[\"crash_id_list\"]),len(partial_data_info_origin[\"safe_id_list\"]))\n",
    "with open(folder+\"/offline_av_alldata_new.json\", \"w\") as fp:\n",
    "    ujson.dump(partial_data_info_origin, fp)\n",
    "    \n",
    "plt.figure()\n",
    "weight_list = np.array(partial_data_info_origin[\"crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weight_list = np.array(partial_data_info_origin[\"safe_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weight_list = np.array(partial_data_info_origin[\"safe_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10 for i in range(11)])\n",
    "print(sum(weight_list>3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bdb2f",
   "metadata": {},
   "source": [
    "### Step 3: Analyze the near-miss events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef087f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate center distance between vehicles\n",
    "veh_length = 5.0\n",
    "veh_width = 1.8\n",
    "circle_r = 1.227\n",
    "tem_len = math.sqrt(circle_r**2-(veh_width/2)**2)\n",
    "\n",
    "def read_json_fcd_from_json(fcd_file, safe_data):\n",
    "    \"\"\"Load the trajectory data from the json file.\n",
    "    \n",
    "    Args:\n",
    "        fcd_file (str): Path to the fcd file.\n",
    "        safe_data (list): List of safe data.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of trajectory data.\n",
    "    \"\"\"\n",
    "    safe_id_list = [ep[0] for ep in safe_data]\n",
    "    fcdjson_obj_list = []\n",
    "    with open(fcd_file) as fp:\n",
    "        for line in fp:\n",
    "            fcdjson_obj = json.loads(line)\n",
    "            ep_id = int(fcdjson_obj[\"original_name\"])\n",
    "            if ep_id in safe_id_list:\n",
    "                fcdjson_obj_list.append(fcdjson_obj)\n",
    "    return fcdjson_obj_list\n",
    "\n",
    "def find_three_circle_centers(veh_info):\n",
    "    \"\"\"Find the centers of the three circles which can cover the vehicle.\n",
    "\n",
    "    Args:\n",
    "        veh_info (dict): Vehicle information.\n",
    "\n",
    "    Returns:\n",
    "        list: List of the centers of the three circles.\n",
    "    \"\"\"\n",
    "    x, y = float(veh_info[\"@x\"]), float(veh_info[\"@y\"])\n",
    "    heading = float(veh_info[\"@angle\"])/180*math.pi\n",
    "    center1 = (\n",
    "        x-veh_length/2*math.sin(heading), \n",
    "        y-veh_length/2*math.cos(heading)\n",
    "    )\n",
    "    center0 = (\n",
    "        center1[0]+(veh_length/2-tem_len)*math.sin(heading),\n",
    "        center1[1]+(veh_length/2-tem_len)*math.cos(heading)\n",
    "    )\n",
    "    center2 = (\n",
    "        center1[0]-(veh_length/2-tem_len)*math.sin(heading),\n",
    "        center1[1]-(veh_length/2-tem_len)*math.cos(heading)\n",
    "    )\n",
    "    center_list = [center0, center1, center2]\n",
    "    return center_list\n",
    "\n",
    "def get_smallest_dist(fcdjson_obj, safe_data):\n",
    "    \"\"\"Get the smallest distance between the centers of the vehicless covering circles.\n",
    "\n",
    "    Args:\n",
    "        fcdjson_obj (dict): Trajectory data.\n",
    "        safe_data (list): List of safe data.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Smallest distance and the episode id.\n",
    "    \"\"\"\n",
    "    traj_info = fcdjson_obj[\"fcd-export\"]\n",
    "    safe_id_list = [ep[0] for ep in safe_data]\n",
    "    assert(int(fcdjson_obj[\"original_name\"])in safe_id_list)\n",
    "    index = safe_id_list.index(int(fcdjson_obj[\"original_name\"]))\n",
    "    clip = (safe_data[index][4],safe_data[index][5])\n",
    "    min_cc_distance_list_time = []\n",
    "    for m in traj_info[\"timestep\"]:\n",
    "        t = int(round(float(m[\"@time\"]),1)*10)\n",
    "        if t < clip[0] or t > clip[1]:\n",
    "            continue\n",
    "        vehs = m[\"vehicle\"]\n",
    "        assert(vehs[-1][\"@id\"]==\"CAV\")\n",
    "        cav_info = vehs[-1]\n",
    "        cav_three_circles = find_three_circle_centers(cav_info)\n",
    "        min_cc_distance_list_vehicles = []\n",
    "        for bv in vehs:\n",
    "            if bv[\"@id\"] == \"CAV\":\n",
    "                continue\n",
    "            bv_three_circles = find_three_circle_centers(bv)\n",
    "            cc_distance_list = []\n",
    "            for cav_c in cav_three_circles:\n",
    "                for bv_c in bv_three_circles:\n",
    "                    cc_distance_list.append(math.sqrt((cav_c[0]-bv_c[0])**2+(cav_c[1]-bv_c[1])**2))\n",
    "            min_cc_distance_list_vehicles.append(min(cc_distance_list))\n",
    "        min_cc_distance_list_time.append(min(min_cc_distance_list_vehicles))\n",
    "    if min_cc_distance_list_time == []:\n",
    "        print(safe_data[index],len(traj_info[\"timestep\"]))\n",
    "        min_cc_distance_list_time = [100]\n",
    "    return min(min_cc_distance_list_time), int(fcdjson_obj[\"original_name\"])\n",
    "\n",
    "def main(fcd_files, index, all_data_path):\n",
    "    \"\"\"Main function for the safety-critical event analysis. We will calculate the minimum distance between the centers of the vehicles covering circles.\n",
    "\n",
    "    Args:\n",
    "        fcd_files (list): List of fcd files.\n",
    "        index (int): Index of the process.\n",
    "        all_data_path (str): Path to the data file.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with open(all_data_path) as fp:\n",
    "        all_data = json.load(fp)\n",
    "        safe_data = all_data[\"safe_id_list\"]\n",
    "    for fcd_file in tqdm(fcd_files):\n",
    "        fcdjson_obj_list = read_json_fcd_from_json(fcd_file, safe_data)\n",
    "        for fcdjson_obj in fcdjson_obj_list:\n",
    "            min_dist, ep_id = get_smallest_dist(fcdjson_obj, safe_data)\n",
    "            results.append([min_dist,ep_id])\n",
    "    print(len(results))\n",
    "    results_plot = np.array(results)[:,0]\n",
    "    plt.figure(dpi=100)\n",
    "    plt.hist(results_plot, bins=100)\n",
    "    plt.show()\n",
    "    with open(f\"{processed_data_tmp_folder_path}/min_center_distance_{index}.npy\", 'wb') as f:\n",
    "        print(f\"{processed_data_tmp_folder_path}/min_center_distance_{index}.npy\")\n",
    "        np.save(f, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68956afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smallest distance for three circles\n",
    "all_data_path = folder+\"/offline_av_alldata_new.json\"\n",
    "tested_and_safe_folder = folder+\"/tested_and_safe/\"\n",
    "safe_fcd_files = sorted(glob.glob(tested_and_safe_folder+\"*.fcd.json\"))\n",
    "print(len(safe_fcd_files))\n",
    "num_each = int(len(safe_fcd_files)/20)+1\n",
    "p_list = []\n",
    "for i in range(20):\n",
    "    p = Process(target=main, args=(safe_fcd_files[i*num_each:(i+1)*num_each], i, all_data_path))\n",
    "    p_list.append(p)\n",
    "for p_ind in p_list:\n",
    "    p_ind.start()\n",
    "for p_ind in p_list:\n",
    "    p_ind.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c82e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find near-miss, minimum three-circle-distance<2.5\n",
    "np_files = glob.glob(f\"{processed_data_tmp_folder_path}/min_center_distance_*.npy\")\n",
    "print(np_files)\n",
    "\n",
    "results_min_dist = []\n",
    "ep_id_list = []\n",
    "for f in np_files:\n",
    "    new_results = np.load(f)\n",
    "    results_min_dist = np.append(results_min_dist, new_results[:,0])\n",
    "    ep_id_list = np.append(ep_id_list, new_results[:,1])\n",
    "print(len(results_min_dist))\n",
    "\n",
    "file_name_alldata = \"offline_av_alldata_new.json\"\n",
    "\n",
    "with open(os.path.join(folder, file_name_alldata)) as fp1:\n",
    "    summary_info = ujson.load(fp1)\n",
    "\n",
    "dist_threshold = 2.5\n",
    "output_data = {\n",
    "    \"safe2crash_id_list\": [],\n",
    "    \"safe2crash_weight\": [],\n",
    "    \"safe2crash_ep_info\": [],\n",
    "}\n",
    "safe2crash_id_list = []\n",
    "safe2crash_weight = []\n",
    "safe_id_list = [info[0] for info in summary_info[\"safe_id_list\"]]\n",
    "for i in tqdm(range(len(ep_id_list))):\n",
    "    if results_min_dist[i] < dist_threshold:\n",
    "        ep_id = ep_id_list[i]\n",
    "        try:\n",
    "            j = safe_id_list.index(ep_id)\n",
    "        except:\n",
    "            print(ep_id)\n",
    "            continue\n",
    "        ep_info = summary_info[\"safe_id_list\"][j]\n",
    "        output_data[\"safe2crash_id_list\"].append(ep_info)\n",
    "        output_data[\"safe2crash_weight\"].append(summary_info[\"safe_weight\"][j])\n",
    "        output_data[\"safe2crash_ep_info\"].append(tuple(ep_info+[summary_info[\"safe_weight\"][j]]))\n",
    "print(len(output_data[\"safe2crash_id_list\"]), len(output_data[\"safe2crash_weight\"]))\n",
    "\n",
    "file_name_output = \"offline_av_nearmiss_new.json\"\n",
    "with open(os.path.join(folder, file_name_output), \"w\") as fp2:\n",
    "    ujson.dump(output_data, fp2)\n",
    "    \n",
    "plt.figure()\n",
    "weight_list = np.array(output_data[\"safe2crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weight_list = np.array(output_data[\"safe2crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10 for i in range(11)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0922afde",
   "metadata": {},
   "source": [
    "### Step 4: Store the processed inforamtion of all safety critical events including crashes and near-miss events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = folder+\"/offline_av_alldata_new.json\"\n",
    "with open(json_file_path) as fp:\n",
    "    data_info_origin = ujson.load(fp)\n",
    "    print(len(data_info_origin[\"crash_id_list\"]))\n",
    "    print(len(data_info_origin[\"safe_id_list\"]))\n",
    "\n",
    "json_file_path = folder+\"/offline_av_nearmiss_new.json\"\n",
    "with open(json_file_path) as fp:\n",
    "    data_info = ujson.load(fp)\n",
    "    print(len(data_info[\"safe2crash_id_list\"]))\n",
    "print(data_info.keys())\n",
    "data_info_new = {}\n",
    "data_info_new[\"crash_id_list\"] = data_info_origin[\"crash_id_list\"]\n",
    "data_info_new[\"crash_weight\"] = data_info_origin[\"crash_weight\"]\n",
    "data_info_new[\"crash_ep_info\"] = data_info_origin[\"crash_ep_info\"]\n",
    "del_index_list = []\n",
    "for i in range(len(data_info[\"safe2crash_id_list\"])):\n",
    "    if data_info[\"safe2crash_id_list\"][i] not in data_info_origin[\"safe_id_list\"]:\n",
    "        print(i)\n",
    "        del_index_list.append(i)\n",
    "for i in del_index_list:\n",
    "    data_info[\"safe2crash_id_list\"].pop(i)\n",
    "    data_info[\"safe2crash_weight\"].pop(i)\n",
    "    data_info[\"safe2crash_ep_info\"].pop(i)\n",
    "print(len(data_info[\"safe2crash_id_list\"]))\n",
    "data_info_new[\"safe2crash_id_list\"] = data_info[\"safe2crash_id_list\"]\n",
    "data_info_new[\"safe2crash_weight\"] = data_info[\"safe2crash_weight\"]\n",
    "data_info_new[\"safe2crash_ep_info\"] = data_info[\"safe2crash_ep_info\"]\n",
    "data_info_new[\"crashnearmiss_history\"] = {}\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_id_list\"] = data_info_origin[\"crash_id_list\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_weight\"] = data_info_origin[\"crash_weight\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_ep_info\"] = data_info_origin[\"crash_ep_info\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_score_list\"] = [[1,0,0,0]]*len(data_info_origin[\"crash_id_list\"])\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_id_list\"] = data_info[\"safe2crash_id_list\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_weight\"] = data_info[\"safe2crash_weight\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_ep_info\"] = data_info[\"safe2crash_ep_info\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_score_list\"] = [[0,1,0,1]]*len(data_info[\"safe2crash_id_list\"])\n",
    "\n",
    "json_file_path_new = folder+\"/offline_av_neweval_crashnearmiss_new_origin.json\"\n",
    "with open(json_file_path_new, \"w\") as fp2:\n",
    "    ujson.dump(data_info_new, fp2)\n",
    "    \n",
    "json_file_path_new = folder+\"/offline_av_neweval_crashnearmiss_new.json\"\n",
    "with open(json_file_path_new, \"w\") as fp2:\n",
    "    ujson.dump(data_info_new, fp2)\n",
    "    \n",
    "plt.figure()\n",
    "weight_list = np.array(data_info_new[\"crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)],alpha=0.5,label=\"crash\")\n",
    "weight_list = np.array(data_info_new[\"safe2crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)],alpha=0.5,label=\"near-miss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10 for i in range(9)],alpha=0.5,label=\"near-miss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
