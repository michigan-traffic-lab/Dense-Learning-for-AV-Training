{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import json, ujson\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Process, Manager, Pool\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please give the path to the data folder\n",
    "root_folder = \"/absolute/path/to/data/folder\"\n",
    "\n",
    "# please give the experiment name of the evaluation\n",
    "exp_name = \"Experiment-testing_DateOfRunning\"\n",
    "\n",
    "processed_data_folder = os.path.join(root_folder, exp_name, \"densified_exps\")\n",
    "original_summary_file = os.path.join(processed_data_folder, \"offline_av_alldata_new.json\")\n",
    "processed_data_tmp_folder_path = \"processed_data_tmp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation study: No episodic data densification (NEDD)\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_alldata_new.json\")) as fp:\n",
    "    data_info_origin = ujson.load(fp)\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_alldata_new_ablationstudy_NEDD.json\"), \"w\") as fp2:\n",
    "    ujson.dump(data_info_origin, fp2)\n",
    "\n",
    "with open(original_summary_file) as fp:\n",
    "    data_info_origin = ujson.load(fp)\n",
    "    print(len(data_info_origin[\"crash_id_list\"]))\n",
    "    print(len(data_info_origin[\"safe_id_list\"]))\n",
    "\n",
    "data_info_new = {}\n",
    "data_info_new[\"crash_id_list\"] = data_info_origin[\"crash_id_list\"]\n",
    "data_info_new[\"crash_weight\"] = [1 for _ in data_info_origin[\"crash_weight\"]]\n",
    "data_info_new[\"crash_ep_info\"] = data_info_origin[\"crash_ep_info\"]\n",
    "\n",
    "data_info_new[\"safe2crash_id_list\"] = data_info_origin[\"safe_id_list\"]\n",
    "data_info_new[\"safe2crash_weight\"] = [1 for _ in data_info_origin[\"safe_weight\"]]\n",
    "data_info_new[\"safe2crash_ep_info\"] = data_info_origin[\"safe_ep_info\"]\n",
    "data_info_new[\"crashnearmiss_history\"] = {}\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_id_list\"] = data_info_origin[\"crash_id_list\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_weight\"] = [1 for _ in data_info_origin[\"crash_weight\"]]\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_ep_info\"] = data_info_origin[\"crash_ep_info\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_score_list\"] = [[1,0,0,0]]*len(data_info_origin[\"crash_id_list\"])\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_id_list\"] = data_info_origin[\"safe_id_list\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_weight\"] = [1 for _ in data_info_origin[\"safe_weight\"]]\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_ep_info\"] = data_info_origin[\"safe_ep_info\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_score_list\"] = [[0,1,0,1]]*len(data_info_origin[\"safe_id_list\"])\n",
    "\n",
    "json_file_path_new = os.path.join(root_folder, exp_name, \"densified_exps\", \"offline_av_neweval_crashnearmiss_new_ablationstudy_NEDD.json\")\n",
    "with open(json_file_path_new, \"w\") as fp2:\n",
    "    ujson.dump(data_info_new, fp2)\n",
    "    \n",
    "plt.figure()\n",
    "weight_list = np.array(data_info_new[\"crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)],alpha=0.5,label=\"crash\")\n",
    "weight_list = np.array(data_info_new[\"safe2crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)],alpha=0.5,label=\"near-miss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10 for i in range(9)],alpha=0.5,label=\"near-miss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate center distance between vehicles\n",
    "veh_length = 5.0\n",
    "veh_width = 1.8\n",
    "circle_r = 1.227\n",
    "tem_len = math.sqrt(circle_r**2-(veh_width/2)**2)\n",
    "\n",
    "def read_json_fcd_from_json(fcd_file, safe_data):\n",
    "    \"\"\"Load the trajectory data from the json file.\n",
    "    \n",
    "    Args:\n",
    "        fcd_file (str): Path to the fcd file.\n",
    "        safe_data (list): List of safe data.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of trajectory data.\n",
    "    \"\"\"\n",
    "    safe_id_list = [ep[0] for ep in safe_data]\n",
    "    fcdjson_obj_list = []\n",
    "    with open(fcd_file) as fp:\n",
    "        for line in fp:\n",
    "            fcdjson_obj = json.loads(line)\n",
    "            ep_id = int(fcdjson_obj[\"original_name\"])\n",
    "            if ep_id in safe_id_list:\n",
    "                fcdjson_obj_list.append(fcdjson_obj)\n",
    "    return fcdjson_obj_list\n",
    "\n",
    "def find_three_circle_centers(veh_info):\n",
    "    \"\"\"Find the centers of the three circles which can cover the vehicle.\n",
    "\n",
    "    Args:\n",
    "        veh_info (dict): Vehicle information.\n",
    "\n",
    "    Returns:\n",
    "        list: List of the centers of the three circles.\n",
    "    \"\"\"\n",
    "    x, y = float(veh_info[\"@x\"]), float(veh_info[\"@y\"])\n",
    "    heading = float(veh_info[\"@angle\"])/180*math.pi\n",
    "    center1 = (\n",
    "        x-veh_length/2*math.sin(heading), \n",
    "        y-veh_length/2*math.cos(heading)\n",
    "    )\n",
    "    center0 = (\n",
    "        center1[0]+(veh_length/2-tem_len)*math.sin(heading),\n",
    "        center1[1]+(veh_length/2-tem_len)*math.cos(heading)\n",
    "    )\n",
    "    center2 = (\n",
    "        center1[0]-(veh_length/2-tem_len)*math.sin(heading),\n",
    "        center1[1]-(veh_length/2-tem_len)*math.cos(heading)\n",
    "    )\n",
    "    center_list = [center0, center1, center2]\n",
    "    return center_list\n",
    "\n",
    "def get_smallest_dist(fcdjson_obj, safe_data):\n",
    "    \"\"\"Get the smallest distance between the centers of the vehicless covering circles.\n",
    "\n",
    "    Args:\n",
    "        fcdjson_obj (dict): Trajectory data.\n",
    "        safe_data (list): List of safe data.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Smallest distance and the episode id.\n",
    "    \"\"\"\n",
    "    traj_info = fcdjson_obj[\"fcd-export\"]\n",
    "    safe_id_list = [ep[0] for ep in safe_data]\n",
    "    assert(int(fcdjson_obj[\"original_name\"])in safe_id_list)\n",
    "    index = safe_id_list.index(int(fcdjson_obj[\"original_name\"]))\n",
    "    clip = (safe_data[index][4],safe_data[index][5])\n",
    "    min_cc_distance_list_time = []\n",
    "    for m in traj_info[\"timestep\"]:\n",
    "        t = int(round(float(m[\"@time\"]),1)*10)\n",
    "        if t < clip[0] or t > clip[1]:\n",
    "            continue\n",
    "        vehs = m[\"vehicle\"]\n",
    "        assert(vehs[-1][\"@id\"]==\"CAV\")\n",
    "        cav_info = vehs[-1]\n",
    "        cav_three_circles = find_three_circle_centers(cav_info)\n",
    "        min_cc_distance_list_vehicles = []\n",
    "        for bv in vehs:\n",
    "            if bv[\"@id\"] == \"CAV\":\n",
    "                continue\n",
    "            bv_three_circles = find_three_circle_centers(bv)\n",
    "            cc_distance_list = []\n",
    "            for cav_c in cav_three_circles:\n",
    "                for bv_c in bv_three_circles:\n",
    "                    cc_distance_list.append(math.sqrt((cav_c[0]-bv_c[0])**2+(cav_c[1]-bv_c[1])**2))\n",
    "            min_cc_distance_list_vehicles.append(min(cc_distance_list))\n",
    "        min_cc_distance_list_time.append(min(min_cc_distance_list_vehicles))\n",
    "    if min_cc_distance_list_time == []:\n",
    "        print(safe_data[index],len(traj_info[\"timestep\"]))\n",
    "        min_cc_distance_list_time = [100]\n",
    "    return min(min_cc_distance_list_time), int(fcdjson_obj[\"original_name\"])\n",
    "\n",
    "def main(fcd_files, index, all_data_path):\n",
    "    \"\"\"Main function for the safety-critical event analysis. We will calculate the minimum distance between the centers of the vehicles covering circles.\n",
    "\n",
    "    Args:\n",
    "        fcd_files (list): List of fcd files.\n",
    "        index (int): Index of the process.\n",
    "        all_data_path (str): Path to the data file.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with open(all_data_path) as fp:\n",
    "        all_data = json.load(fp)\n",
    "        safe_data = all_data[\"safe_id_list\"]\n",
    "    for fcd_file in tqdm(fcd_files):\n",
    "        fcdjson_obj_list = read_json_fcd_from_json(fcd_file, safe_data)\n",
    "        for fcdjson_obj in fcdjson_obj_list:\n",
    "            min_dist, ep_id = get_smallest_dist(fcdjson_obj, safe_data)\n",
    "            results.append([min_dist,ep_id])\n",
    "    print(len(results))\n",
    "    results_plot = np.array(results)[:,0]\n",
    "    plt.figure(dpi=100)\n",
    "    plt.hist(results_plot, bins=100)\n",
    "    plt.show()\n",
    "    with open(f\"{processed_data_tmp_folder_path}/min_center_distance_ablationstudy_NSLDD_{index}.npy\", 'wb') as f:\n",
    "        print(f\"{processed_data_tmp_folder_path}/min_center_distance_ablationstudy_NSLDD_{index}.npy\")\n",
    "        np.save(f, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation study: No state-level data densification (NSLDD)\n",
    "def find_training_range(crit_info):\n",
    "    t_list = []\n",
    "    for info in crit_info:\n",
    "        t_list.append(info[0])\n",
    "    return t_list[0], t_list[-1]\n",
    "    \n",
    "\n",
    "files = sorted(glob.glob(os.path.join(processed_data_tmp_folder_path, \"complete_info_offlinecollect-tmp_*\")))\n",
    "# print(files)\n",
    "num_crash = 0\n",
    "num_safe = 0\n",
    "\n",
    "data_info_origin = {\n",
    "    \"safe_id_list\": [], \"safe_weight\": [], \"safe_ep_info\": [],\n",
    "    \"crash_id_list\": [], \"crash_weight\": [], \"crash_ep_info\": [],\n",
    "}\n",
    "\n",
    "int_index = 5\n",
    "ablationstudy_original_summary_file = os.path.join(processed_data_folder, \"offline_av_alldata_new_ablationstudy_NSLDD.json\")\n",
    "critic_len_list = []\n",
    "\n",
    "\n",
    "for file in tqdm(files):\n",
    "#     print(file)\n",
    "    with open(file) as fp:\n",
    "        json_obj = json.load(fp)\n",
    "#     print(json_obj[\"safe_id_list\"])\n",
    "#     print(json_obj[\"crash_id_list\"][:100])\n",
    "    for i in range(len(json_obj[\"safe_id_list\"])):\n",
    "        info = json_obj[\"safe_id_list\"][i]\n",
    "        if len(info[int_index]) != 0:\n",
    "            re = find_training_range(info[8])\n",
    "#             debug(info)\n",
    "            if re[0] is None:\n",
    "                num_safe+=1\n",
    "            else:\n",
    "                info_out = [\n",
    "                    info[0], info[1], info[2], info[3],\n",
    "                    int(float(re[0])*10), int(float(re[1])*10)\n",
    "                ]\n",
    "#                 print(info_out)\n",
    "                data_info_origin[\"safe_id_list\"].append(info_out)\n",
    "                data_info_origin[\"safe_weight\"].append(json_obj[\"safe_weight\"][i])\n",
    "                data_info_origin[\"safe_ep_info\"].append(tuple(info_out+[json_obj[\"safe_weight\"][i]]))\n",
    "               \n",
    "    for i in range(len(json_obj[\"crash_id_list\"])):\n",
    "        info = json_obj[\"crash_id_list\"][i]\n",
    "        if len(info[int_index]) != 0:\n",
    "            re = find_training_range(info[8])\n",
    "#             debug(info)\n",
    "            if re[0] is None:\n",
    "                num_crash+=1\n",
    "                print(\"remove\")\n",
    "            else:\n",
    "                info_out = [\n",
    "                    info[0], info[1], info[2], info[3],\n",
    "                    int(float(re[0])*10), int(float(re[1])*10)\n",
    "                ]\n",
    "#                 print(info_out)\n",
    "                data_info_origin[\"crash_id_list\"].append(info_out)\n",
    "                data_info_origin[\"crash_weight\"].append(json_obj[\"crash_weight\"][i])\n",
    "                data_info_origin[\"crash_ep_info\"].append(tuple(info_out+[json_obj[\"crash_weight\"][i]]))\n",
    "#                 critic_len_list.append(ranges[-1][-1]-ranges[-1][0]+1)\n",
    "       \n",
    "print(num_safe, num_crash)\n",
    "print(np.mean(critic_len_list))\n",
    "\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_alldata_ablationstudy_NSLDD.json\"), \"w\") as fp:\n",
    "    ujson.dump(data_info_origin, fp)\n",
    "\n",
    "filtered_ep_id = []\n",
    "\n",
    "print(len(data_info_origin[\"crash_id_list\"]))\n",
    "print(data_info_origin.keys())\n",
    "print(len(data_info_origin[\"safe_id_list\"]))\n",
    "partial_data_info_origin = {}\n",
    "partial_data_info_origin[\"crash_id_list\"] = []\n",
    "partial_data_info_origin[\"crash_weight\"] = []\n",
    "partial_data_info_origin[\"crash_ep_info\"] = []\n",
    "partial_data_info_origin[\"safe_id_list\"] = []\n",
    "partial_data_info_origin[\"safe_weight\"] = []\n",
    "partial_data_info_origin[\"safe_ep_info\"] = []\n",
    "for i in range(len(data_info_origin[\"crash_id_list\"])):\n",
    "    if data_info_origin[\"crash_id_list\"][i][0] in filtered_ep_id:\n",
    "        print(\"crash:\", data_info_origin[\"crash_id_list\"][i])\n",
    "        continue\n",
    "    if data_info_origin[\"crash_id_list\"][i][5]-data_info_origin[\"crash_id_list\"][i][4]>=2:\n",
    "        partial_data_info_origin[\"crash_id_list\"].append(data_info_origin[\"crash_id_list\"][i])\n",
    "        partial_data_info_origin[\"crash_weight\"].append(data_info_origin[\"crash_weight\"][i])\n",
    "        partial_data_info_origin[\"crash_ep_info\"].append(data_info_origin[\"crash_ep_info\"][i])\n",
    "    else:\n",
    "        print(data_info_origin[\"crash_id_list\"][i])\n",
    "for i in range(len(data_info_origin[\"safe_id_list\"])):\n",
    "    if data_info_origin[\"safe_id_list\"][i][0] in filtered_ep_id:\n",
    "        print(\"safe:\",data_info_origin[\"safe_id_list\"][i])\n",
    "        continue\n",
    "    if data_info_origin[\"safe_id_list\"][i][5]-data_info_origin[\"safe_id_list\"][i][4]>=2:\n",
    "        partial_data_info_origin[\"safe_id_list\"].append(data_info_origin[\"safe_id_list\"][i])\n",
    "        partial_data_info_origin[\"safe_weight\"].append(data_info_origin[\"safe_weight\"][i])\n",
    "        partial_data_info_origin[\"safe_ep_info\"].append(data_info_origin[\"safe_ep_info\"][i])\n",
    "print(len(partial_data_info_origin[\"crash_id_list\"]),len(partial_data_info_origin[\"safe_id_list\"]))\n",
    "\n",
    "with open(ablationstudy_original_summary_file, \"w\") as fp:\n",
    "    ujson.dump(partial_data_info_origin, fp)\n",
    "\n",
    "# smallest distance for three circles\n",
    "all_data_path = ablationstudy_original_summary_file\n",
    "tested_and_safe_folder = os.path.join(processed_data_folder,\"tested_and_safe\")\n",
    "print(tested_and_safe_folder)\n",
    "safe_fcd_files = sorted(glob.glob(os.path.join(tested_and_safe_folder,\"*.fcd.json\")))\n",
    "print(\"len(safe_fcd_files)\",len(safe_fcd_files))\n",
    "num_each = int(len(safe_fcd_files)/20)+1\n",
    "p_list = []\n",
    "for i in range(20):\n",
    "    p = Process(target=main, args=(safe_fcd_files[i*num_each:(i+1)*num_each], i, all_data_path))\n",
    "    p_list.append(p)\n",
    "for p_ind in p_list:\n",
    "    p_ind.start()\n",
    "for p_ind in p_list:\n",
    "    p_ind.join()\n",
    "\n",
    "# find near-miss, minimum three-circle-distance<2.5\n",
    "np_files = glob.glob(f\"{processed_data_tmp_folder_path}/min_center_distance_ablationstudy_NSLDD_*.npy\")\n",
    "print(np_files)\n",
    "\n",
    "results_min_dist = []\n",
    "ep_id_list = []\n",
    "for f in np_files:\n",
    "    new_results = np.load(f)\n",
    "    results_min_dist = np.append(results_min_dist, new_results[:,0])\n",
    "    ep_id_list = np.append(ep_id_list, new_results[:,1])\n",
    "print(len(results_min_dist))\n",
    "\n",
    "file_name_alldata = \"offline_av_alldata_new.json\"\n",
    "\n",
    "summary_info = partial_data_info_origin\n",
    "\n",
    "dist_threshold = 2.5\n",
    "output_data = {\n",
    "    \"safe2crash_id_list\": [],\n",
    "    \"safe2crash_weight\": [],\n",
    "    \"safe2crash_ep_info\": [],\n",
    "}\n",
    "safe2crash_id_list = []\n",
    "safe2crash_weight = []\n",
    "safe_id_list = [info[0] for info in summary_info[\"safe_id_list\"]]\n",
    "for i in tqdm(range(len(ep_id_list))):\n",
    "    if results_min_dist[i] < dist_threshold:\n",
    "        ep_id = ep_id_list[i]\n",
    "        try:\n",
    "            j = safe_id_list.index(ep_id)\n",
    "        except:\n",
    "            print(ep_id)\n",
    "            continue\n",
    "        ep_info = summary_info[\"safe_id_list\"][j]\n",
    "        output_data[\"safe2crash_id_list\"].append(ep_info)\n",
    "        output_data[\"safe2crash_weight\"].append(summary_info[\"safe_weight\"][j])\n",
    "        output_data[\"safe2crash_ep_info\"].append(tuple(ep_info+[summary_info[\"safe_weight\"][j]]))\n",
    "print(len(output_data[\"safe2crash_id_list\"]), len(output_data[\"safe2crash_weight\"]))\n",
    "\n",
    "file_name_output = \"offline_av_nearmiss_new_ablationstudy_NSLDD.json\"\n",
    "with open(os.path.join(processed_data_folder, file_name_output), \"w\") as fp2:\n",
    "    ujson.dump(output_data, fp2)\n",
    "\n",
    "with open(ablationstudy_original_summary_file) as fp:\n",
    "    data_info_origin = ujson.load(fp)\n",
    "    print(len(data_info_origin[\"crash_id_list\"]))\n",
    "    print(len(data_info_origin[\"safe_id_list\"]))\n",
    "\n",
    "nearmiss_json_file_path = os.path.join(processed_data_folder, \"offline_av_nearmiss_new_ablationstudy_NSLDD.json\")\n",
    "\n",
    "with open(nearmiss_json_file_path) as fp:\n",
    "    data_info = ujson.load(fp)\n",
    "    print(len(data_info[\"safe2crash_id_list\"]))\n",
    "print(data_info.keys())\n",
    "\n",
    "data_info_new = {}\n",
    "data_info_new[\"crash_id_list\"] = data_info_origin[\"crash_id_list\"]\n",
    "data_info_new[\"crash_weight\"] = data_info_origin[\"crash_weight\"]\n",
    "data_info_new[\"crash_ep_info\"] = data_info_origin[\"crash_ep_info\"]\n",
    "del_index_list = []\n",
    "for i in tqdm(range(len(data_info[\"safe2crash_id_list\"]))):\n",
    "    if data_info[\"safe2crash_id_list\"][i] not in data_info_origin[\"safe_id_list\"]:\n",
    "        print(i,data_info[\"safe2crash_id_list\"][i])\n",
    "        del_index_list.append(i)\n",
    "for i in del_index_list:\n",
    "    data_info[\"safe2crash_id_list\"].pop(i)\n",
    "    data_info[\"safe2crash_weight\"].pop(i)\n",
    "    data_info[\"safe2crash_ep_info\"].pop(i)\n",
    "print(len(data_info[\"safe2crash_id_list\"]))\n",
    "data_info_new[\"safe2crash_id_list\"] = data_info[\"safe2crash_id_list\"]\n",
    "data_info_new[\"safe2crash_weight\"] = data_info[\"safe2crash_weight\"]\n",
    "data_info_new[\"safe2crash_ep_info\"] = data_info[\"safe2crash_ep_info\"]\n",
    "data_info_new[\"crashnearmiss_history\"] = {}\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_id_list\"] = data_info_origin[\"crash_id_list\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_weight\"] = data_info_origin[\"crash_weight\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_ep_info\"] = data_info_origin[\"crash_ep_info\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_score_list\"] = [[1,0,0,0]]*len(data_info_origin[\"crash_id_list\"])\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_id_list\"] = data_info[\"safe2crash_id_list\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_weight\"] = data_info[\"safe2crash_weight\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_ep_info\"] = data_info[\"safe2crash_ep_info\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_score_list\"] = [[0,1,0,1]]*len(data_info[\"safe2crash_id_list\"])\n",
    "\n",
    "json_file_path_new = os.path.join(processed_data_folder, \"offline_av_neweval_crashnearmiss_new_ablationstudy_NSLDD.json\")\n",
    "with open(json_file_path_new, \"w\") as fp2:\n",
    "    ujson.dump(data_info_new, fp2)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure()\n",
    "weight_list = np.array(data_info_new[\"crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)],alpha=0.5,label=\"crash\")\n",
    "weight_list = np.array(data_info_new[\"safe2crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)],alpha=0.5,label=\"near-miss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10 for i in range(9)],alpha=0.5,label=\"near-miss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation study: No near-miss episodes (NNME)\n",
    "with open(original_summary_file) as fp:\n",
    "    data_info_origin = ujson.load(fp)\n",
    "    \n",
    "    print(len(data_info_origin[\"crash_id_list\"]))\n",
    "    print(len(data_info_origin[\"safe_id_list\"]))\n",
    "\n",
    "data_info_origin_new = copy.deepcopy(data_info_origin)\n",
    "\n",
    "json_file_path_new = os.path.join(processed_data_folder, \"offline_av_alldata_new_ablationstudy_NNME.json\")\n",
    "with open(json_file_path_new, \"w\") as fp2:\n",
    "    ujson.dump(data_info_origin_new, fp2)\n",
    "    \n",
    "data_info_new = {}\n",
    "data_info_new[\"crash_id_list\"] = data_info_origin[\"crash_id_list\"]\n",
    "data_info_new[\"crash_weight\"] = data_info_origin[\"crash_weight\"]\n",
    "data_info_new[\"crash_ep_info\"] = data_info_origin[\"crash_ep_info\"]\n",
    "\n",
    "data_info_new[\"safe2crash_id_list\"] = []\n",
    "data_info_new[\"safe2crash_weight\"] = []\n",
    "data_info_new[\"safe2crash_ep_info\"] = []\n",
    "data_info_new[\"crashnearmiss_history\"] = {}\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_id_list\"] = data_info_origin[\"crash_id_list\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_weight\"] = data_info_origin[\"crash_weight\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_ep_info\"] = data_info_origin[\"crash_ep_info\"]\n",
    "data_info_new[\"crashnearmiss_history\"][\"crash_score_list\"] = [[1,0,0,0]]*len(data_info_origin[\"crash_id_list\"])\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_id_list\"] = []\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_weight\"] = []\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_ep_info\"] = []\n",
    "data_info_new[\"crashnearmiss_history\"][\"safe2crash_score_list\"] = []\n",
    "\n",
    "json_file_path_new = os.path.join(processed_data_folder, \"offline_av_neweval_crashnearmiss_new_ablationstudy_NNME.json\")\n",
    "with open(json_file_path_new, \"w\") as fp2:\n",
    "    ujson.dump(data_info_new, fp2)\n",
    "    \n",
    "plt.figure()\n",
    "weight_list = np.array(data_info_new[\"crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)],alpha=0.5,label=\"crash\")\n",
    "weight_list = np.array(data_info_new[\"safe2crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)],alpha=0.5,label=\"near-miss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10 for i in range(9)],alpha=0.5,label=\"near-miss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation study: No retrospective data densification (NRDD)\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_alldata_new.json\")) as fp:\n",
    "    data_info_origin = ujson.load(fp)\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_alldata_new_ablationstudy_NRDD.json\"), \"w\") as fp2:\n",
    "    ujson.dump(data_info_origin, fp2)\n",
    "\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_neweval_crashnearmiss_new_origin.json\")) as fp:\n",
    "    data_info_new = ujson.load(fp)\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_neweval_crashnearmiss_new_ablationstudy_NRDD.json\"), \"w\") as fp2:\n",
    "    ujson.dump(data_info_new, fp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation study: No near-miss episodes and retrospective data densification (NNME_NRDD)\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_alldata_new.json\")) as fp:\n",
    "    data_info_origin = ujson.load(fp)\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_alldata_new_ablationstudy_NNME_NRDD.json\"), \"w\") as fp2:\n",
    "    ujson.dump(data_info_origin, fp2)\n",
    "\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_neweval_crashnearmiss_new_origin.json\")) as fp:\n",
    "    data_info_new = ujson.load(fp)\n",
    "    data_info_new[\"safe2crash_id_list\"] = []\n",
    "    data_info_new[\"safe2crash_weight\"] = []\n",
    "    data_info_new[\"safe2crash_ep_info\"] = []\n",
    "    data_info_new[\"crashnearmiss_history\"][\"safe2crash_id_list\"] = []\n",
    "    data_info_new[\"crashnearmiss_history\"][\"safe2crash_weight\"] = []\n",
    "    data_info_new[\"crashnearmiss_history\"][\"safe2crash_ep_info\"] = []\n",
    "    data_info_new[\"crashnearmiss_history\"][\"safe2crash_score_list\"] = []\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_neweval_crashnearmiss_new_ablationstudy_NNME_NRDD.json\"), \"w\") as fp2:\n",
    "    ujson.dump(data_info_new, fp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation study: No trajectory resampling by probability of occurrence in NDE (NRNDE)\n",
    "with open(original_summary_file) as fp:\n",
    "    data_info_origin = ujson.load(fp)\n",
    "    print(len(data_info_origin[\"crash_id_list\"]))\n",
    "    print(len(data_info_origin[\"safe_id_list\"]))\n",
    "    for i in range(len(data_info_origin[\"crash_id_list\"])):\n",
    "        data_info_origin[\"crash_weight\"][i] = 1.\n",
    "        data_info_origin[\"crash_ep_info\"][i][-1] = 1.\n",
    "    for i in range(len(data_info_origin[\"safe_id_list\"])):\n",
    "        data_info_origin[\"safe_weight\"][i] = 1.\n",
    "        data_info_origin[\"safe_ep_info\"][i][-1] = 1.\n",
    "\n",
    "json_file_path_new = os.path.join(processed_data_folder, \"offline_av_alldata_new_ablationstudy_NRNDE.json\")\n",
    "with open(json_file_path_new, \"w\") as fp2:\n",
    "    ujson.dump(data_info_origin, fp2)\n",
    "\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_neweval_crashnearmiss_new_origin.json\")) as fp:\n",
    "    data_info_new = ujson.load(fp)\n",
    "\n",
    "    print(len(data_info_origin[\"crash_id_list\"]))\n",
    "    print(len(data_info_origin[\"safe_id_list\"]))\n",
    "    for i in range(len(data_info_new[\"crash_id_list\"])):\n",
    "        data_info_new[\"crash_weight\"][i] = 1.\n",
    "        data_info_new[\"crash_ep_info\"][i][-1] = 1.\n",
    "    for i in range(len(data_info_new[\"safe2crash_id_list\"])):\n",
    "        data_info_new[\"safe2crash_weight\"][i] = 1.\n",
    "        data_info_new[\"safe2crash_ep_info\"][i][-1] = 1.\n",
    "    \n",
    "    for i in range(len(data_info_new[\"crashnearmiss_history\"][\"crash_id_list\"])):\n",
    "        data_info_new[\"crashnearmiss_history\"][\"crash_weight\"][i] = 1.\n",
    "        data_info_new[\"crashnearmiss_history\"][\"crash_ep_info\"][i][-1] = 1.\n",
    "    for i in range(len(data_info_new[\"crashnearmiss_history\"][\"safe2crash_id_list\"])):\n",
    "        data_info_new[\"crashnearmiss_history\"][\"safe2crash_weight\"][i] = 1.\n",
    "        data_info_new[\"crashnearmiss_history\"][\"safe2crash_ep_info\"][i][-1] = 1.\n",
    "        \n",
    "json_file_path_new = os.path.join(processed_data_folder, \"offline_av_neweval_crashnearmiss_new_ablationstudy_NRNDE.json\")\n",
    "with open(json_file_path_new, \"w\") as fp2:\n",
    "    ujson.dump(data_info_new, fp2)\n",
    "\n",
    "plt.figure()\n",
    "weight_list = np.array(data_info_new[\"crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)],alpha=0.5,label=\"crash\")\n",
    "weight_list = np.array(data_info_new[\"safe2crash_weight\"])\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10-15 for i in range(81)],alpha=0.5,label=\"near-miss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.hist(np.log10(weight_list[np.nonzero(weight_list)]), bins=[2*i/10 for i in range(9)],alpha=0.5,label=\"near-miss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation study: No reconnection of informative states in Markov process (NRSMDP)\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_alldata_new.json\")) as fp:\n",
    "    data_info_origin = ujson.load(fp)\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_alldata_new_ablationstudy_NRSMDP.json\"), \"w\") as fp2:\n",
    "    ujson.dump(data_info_origin, fp2)\n",
    "\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_neweval_crashnearmiss_new_origin.json\")) as fp:\n",
    "    data_info_new = ujson.load(fp)\n",
    "with open(os.path.join(processed_data_folder, \"offline_av_neweval_crashnearmiss_new_ablationstudy_NRSMDP.json\"), \"w\") as fp2:\n",
    "    ujson.dump(data_info_new, fp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_dlav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
